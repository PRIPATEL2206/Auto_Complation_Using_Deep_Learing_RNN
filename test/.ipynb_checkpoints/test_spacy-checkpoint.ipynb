{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Too', 'expensive', 'I', 'once', 'spend', '200.00', 'on', '30', 'markers'],\n",
       " [\"it's\", 'very', 'functional'],\n",
       " ['Trash', 'app', 'login', 'function', 'now', 'long', 'works.'],\n",
       " ['One', 'of', 'my', 'favorite', 'apps', '🥰'],\n",
       " ['Lost',\n",
       "  'my',\n",
       "  'former',\n",
       "  'purchases',\n",
       "  'and',\n",
       "  'my',\n",
       "  'wish',\n",
       "  'list',\n",
       "  'this',\n",
       "  'was',\n",
       "  'very',\n",
       "  'disappointing',\n",
       "  'they',\n",
       "  'are',\n",
       "  'important',\n",
       "  'to',\n",
       "  'me',\n",
       "  'for',\n",
       "  'future',\n",
       "  'purchases.',\n",
       "  'Other',\n",
       "  'than',\n",
       "  'that',\n",
       "  'I',\n",
       "  'would',\n",
       "  'have',\n",
       "  'given',\n",
       "  'a',\n",
       "  'much',\n",
       "  'better',\n",
       "  'rating.',\n",
       "  'The',\n",
       "  'address',\n",
       "  'is',\n",
       "  'my',\n",
       "  'proper',\n",
       "  'address.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=[]\n",
    "t=['Too expensive I once spend 200.00 on 30 markers',\n",
    " \"it's very functional\",\n",
    " 'Trash app login function now long works.',\n",
    " 'One of my favorite apps 🥰',\n",
    " 'Lost my former purchases and my wish list this was very disappointing they are important to me for future purchases. Other than that I would have given a much better rating. The address is my proper address.']\n",
    "for i in t:\n",
    "    ans.append(i.split(\" \"))\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.3182025 ,  0.0481326 , -0.24693602, -0.53483504, -1.265362  ,\n",
       "       -0.33489054, -0.3785066 ,  0.72888076, -0.4004829 , -1.1298752 ,\n",
       "        2.0605822 , -0.45395228, -0.06097533,  0.55685306, -1.3731496 ,\n",
       "        0.57691896, -0.39081657, -1.8360785 ,  0.32769608,  1.1926407 ,\n",
       "       -0.64532727,  0.03720415,  0.3169452 , -0.948147  ,  0.10798544,\n",
       "        0.31648165, -0.04056926,  0.44128966, -1.6455365 , -0.17855047,\n",
       "       -0.49472767, -1.1990017 ,  0.19053112,  0.29368225,  0.02948284,\n",
       "       -1.0938743 , -0.03218547, -0.7093563 ,  1.2237914 ,  0.7788602 ,\n",
       "       -1.5816326 , -0.39585346, -0.59879607,  0.30390552, -0.45964047,\n",
       "       -0.9530193 ,  1.583483  ,  0.15237147,  0.20513782,  0.5531368 ,\n",
       "        0.02766475,  0.90625   , -0.45493028,  0.610346  , -0.2509181 ,\n",
       "        0.16524261,  2.1668503 , -0.4449928 ,  0.23622361,  0.28387123,\n",
       "       -1.6314783 , -0.39166838,  0.913163  , -0.09041864,  0.04146764,\n",
       "        0.9075119 , -0.37480408,  0.11911365,  1.0861602 ,  0.59701276,\n",
       "        1.7659241 ,  0.14728047, -0.43853006,  0.67647487, -0.9634174 ,\n",
       "        0.62497956, -1.239614  ,  1.208467  , -0.6322193 , -0.44400138,\n",
       "       -0.22864082, -0.26106578, -1.136088  ,  0.3841975 ,  0.60553753,\n",
       "        1.3695571 ,  2.0459454 ,  0.7745363 ,  0.7431768 ,  0.5222726 ,\n",
       "        0.11838442,  0.08769114,  0.9240191 ,  0.46569365, -0.72464013,\n",
       "        0.31605408], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(doc.vector))\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vactore = map(lambda sentence : map(lambda word: nlp(word).vector,sentence),ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "vactore_to_store=[]\n",
    "i=0\n",
    "for sen in final_vactore:\n",
    "    vactore_to_store.append(list(sen))\n",
    "    print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test/test_word_vectore.pkl', 'wb') as wv:\n",
    "    pickle.dump(vactore_to_store, file=wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vactore_to_store=None\n",
    "vactore_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../test/test_word_vectore.pkl',\"rb\") as wv:\n",
    "    vactore_to_store=pickle.load(wv)\n",
    "len(vactore_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vactore_to_store[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenth_of_word=max(map(lambda d: len(d),vactore_to_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6744485 ,  0.09763765, -0.50079966, -0.06556499,  0.9195022 ,\n",
       "       -0.63729703, -0.2899168 ,  1.1459111 ,  0.53100854, -0.21004438,\n",
       "        0.55844146,  0.10453361, -0.806252  ,  0.7025321 ,  0.18101403,\n",
       "        1.2035627 , -0.47432548, -1.2971926 ,  0.29706213,  0.52683884,\n",
       "       -1.1041174 , -0.22762695, -0.7605554 , -0.6105151 ,  0.57130116,\n",
       "       -0.03279816,  0.5363385 ,  1.3503115 , -0.07000907,  0.5348519 ,\n",
       "       -0.9295767 ,  0.502498  ,  0.3945883 ,  0.49527603, -0.99243385,\n",
       "       -0.87142456,  1.9107007 ,  1.5354953 ,  0.6064455 , -0.01512516,\n",
       "        0.61103845, -0.11028257, -1.0777979 , -0.3456978 ,  0.15367143,\n",
       "       -1.1824231 ,  0.08445048,  0.15195109,  1.1250834 , -1.2564957 ,\n",
       "        0.5561664 ,  1.3237829 , -0.04100269, -1.4522994 ,  1.2614506 ,\n",
       "       -0.17155561, -0.561672  , -0.39980853,  1.0015309 ,  0.6632962 ,\n",
       "       -0.75222814,  0.4741918 , -1.0264657 , -1.7922862 ,  1.501074  ,\n",
       "        0.78183615,  0.43971348, -0.30594847, -1.1697135 , -0.22782765,\n",
       "        0.3352064 ,  0.2098656 ,  1.3959275 ,  0.18672577, -0.68322194,\n",
       "        1.0899327 ,  0.23640844, -0.6953238 , -0.4319567 , -0.9877803 ,\n",
       "       -0.70483834, -0.29834634,  0.12374305, -1.1424321 , -0.31717774,\n",
       "        0.49171242, -0.30233458,  0.17769642, -0.8472849 , -0.71978563,\n",
       "        0.08882001,  0.86942613,  2.0116148 ,  0.4296194 ,  0.8082175 ,\n",
       "        0.72387284], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOV_vac=nlp(\"OUTOFVOCAB\").vector\n",
    "OOV_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vactore_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vactore_to_store[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_gen=[]\n",
    "for sen in  vactore_to_store:\n",
    "    prews_cut=-1\n",
    "    for i in range(1,len(sen)):\n",
    "        # print(i)\n",
    "        if i%20==0:\n",
    "            print(i)\n",
    "            prews_cut=i-1\n",
    "        next_word_gen.append(sen[prews_cut:i+1])\n",
    "len(next_word_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_word_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lenth_of_word=max(map(lambda d: len(d),next_word_gen))\n",
    "max_lenth_of_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    next_word_gen,\n",
    "    maxlen=max_lenth_of_word,\n",
    "    padding='pre',\n",
    "    value=OOV_vac\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 18, 96)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E198] Unable to return 1 most similar vectors for the current vectors table, which contains 0 vectors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mOOV_vac\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the input is 1x1 (x300)\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Projects\\aiml\\practice\\aimlenv\\Lib\\site-packages\\spacy\\vectors.pyx:572\u001b[0m, in \u001b[0;36mspacy.vectors.Vectors.most_similar\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E198] Unable to return 1 most similar vectors for the current vectors table, which contains 0 vectors."
     ]
    }
   ],
   "source": [
    "nlp.vocab.vectors.most_similar(\n",
    "        np.asarray([\n",
    "           OOV_vac  # the input is 1x1 (x300)\n",
    "            ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7200140583007839\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baps\\AppData\\Local\\Temp\\ipykernel_28692\\2086562692.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(x.similarity(y))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = nlp(\"man\")\n",
    "y = nlp(\"king\")\n",
    "print(x.similarity(y))\n",
    "print(x.similarity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nuthin\n",
      "ü.\n",
      "p.m\n",
      "Kan\n",
      "Mar\n",
      "When's\n",
      " \n",
      "Sept.\n",
      "c.\n",
      "Mont.\n",
      ":-}\n",
      "12a.m.\n",
      "e.g\n",
      "Why's\n",
      "it\n",
      "6p.m\n",
      "Jr.\n",
      "Who’s\n",
      "K.\n",
      "Calif.\n",
      "e\n",
      "Ill.\n",
      "O'clock\n",
      "o'clock\n",
      "Mich.\n",
      "is\n",
      ":-o\n",
      "n.\n",
      "Might\n",
      "Nov\n",
      ">.<\n",
      "he's\n",
      "it’s\n",
      "where’s\n",
      "Wash.\n",
      "where\n",
      ":-*\n",
      "she's\n",
      "g.\n",
      ":()\n",
      ")-:\n",
      "X\n",
      "S.C.\n",
      "Del\n",
      "Why’s\n",
      "0.o\n",
      "must\n",
      "Goin'\n",
      "4a.m.\n",
      "5p.m.\n",
      "Mass.\n",
      "co.\n",
      "━\n",
      "(-_-)\n",
      "Ariz\n",
      "had\n",
      "0\n",
      "vs.\n",
      "x.\n",
      "><(((*>\n",
      "11a.m.\n",
      "-o\n",
      "When’s\n",
      "Calif\n",
      "does\n",
      "nothin’\n",
      "’S\n",
      "Cos\n",
      "I.e\n",
      "8-)\n",
      "Would\n",
      "do\n",
      "\"\n",
      "Rev.\n",
      "’s\n",
      "N.M.\n",
      "°c.\n",
      "b\n",
      "O.o\n",
      "might\n",
      "q.\n",
      "this's\n",
      "’’\n",
      "Goin’\n",
      "Has\n",
      "N.J.\n",
      "pm\n",
      "ought\n",
      "Dec\n",
      "3p.m\n",
      "<space>\n",
      "Ore.\n",
      "10p.m\n",
      "h.\n",
      "where's\n",
      "doin’\n",
      "What's\n",
      "he’s\n",
      "'cos\n",
      "ä\n",
      "That's\n",
      "11a.m\n",
      "these\n",
      "1p.m\n",
      "Tenn\n",
      ";D\n",
      "Miss\n",
      "Ga\n",
      "Must\n",
      "9p.m\n",
      "somethin\n",
      "Nev.\n",
      "What\n",
      "z.\n",
      "’\n",
      "there's\n",
      "'Cause\n",
      "ü\n",
      "r\n",
      "9\n",
      "Dec.\n",
      "︵\n",
      "an\n",
      "v.s\n",
      "Jun.\n",
      ":-))\n",
      ";_;\n",
      "When\n",
      "D.C.\n",
      "Have\n",
      "You\n",
      "10\n",
      "Where's\n",
      "6a.m\n",
      "o_o\n",
      "O\n",
      "'coz\n",
      "'cause\n",
      "co\n",
      "doin\n",
      "how's\n",
      "i.e\n",
      "._.\n",
      "c’m\n",
      "Prof.\n",
      ":’-)\n",
      "havin'\n",
      "Co.\n",
      "Va.\n",
      "Mt\n",
      "|\n",
      ":((\n",
      "O’clock\n",
      "Let\n",
      "Del.\n",
      "xDD\n",
      "Prof\n",
      "i.e.\n",
      "Ai\n",
      "lovin'\n",
      "when’s\n",
      "Does\n",
      "Minn\n",
      "g\n",
      "12a.m\n",
      "Who's\n",
      "-\n",
      "(\n",
      "La\n",
      "11\n",
      "(;\n",
      "12p.m.\n",
      "Mich\n",
      "n’t\n",
      "_\n",
      "Not\n",
      "1p.m.\n",
      "Okla.\n",
      "Okla\n",
      "what's\n",
      "□\n",
      ":P\n",
      "w\n",
      "cos\n",
      ":'-(\n",
      "Ph.D.\n",
      "It’s\n",
      "Va\n",
      "Nothin’\n",
      "Ltd.\n",
      "would\n",
      ":-/\n",
      "Rep\n",
      "Was\n",
      ";\n",
      "a.\n",
      ":)))\n",
      "k\n",
      "o.\n",
      "XD\n",
      "Ga.\n",
      "Sha\n",
      "C\n",
      "Ought\n",
      "Sep\n",
      ">:o\n",
      ":-)\n",
      "<3\n",
      "d\n",
      "8)\n",
      "Who\n",
      "(¬_¬)\n",
      "Pa.\n",
      "f\n",
      "king\n",
      "V_V\n",
      ":(((\n",
      "need\n",
      "That’s\n",
      "]\n",
      "-8\n",
      "c\n",
      "(^_^)\n",
      "v.s.\n",
      ":D\n",
      "m\n",
      "cause\n",
      "m.\n",
      "C'm\n",
      "8-\n",
      "0_0\n",
      "Somethin’\n",
      "¬_¬\n",
      "you\n",
      "=\n",
      "that’s\n",
      "was\n",
      "’m\n",
      "How’s\n",
      "F\n",
      "what’s\n",
      "‘s\n",
      "y\n",
      ":o\n",
      "Jr\n",
      "Mont\n",
      ":|\n",
      "Where\n",
      ":1\n",
      "nt\n",
      "this’s\n",
      "0.0\n",
      "11p.m\n",
      "Gen\n",
      "why’s\n",
      "Oct.\n",
      "ok\n",
      "Let's\n",
      "=D\n",
      "-0\n",
      ".\n",
      "who\n",
      "Gov.\n",
      "May\n",
      "Nov.\n",
      ":o)\n",
      "Mr.\n",
      "’cos\n",
      "'ve\n",
      "Ky.\n",
      "why\n",
      "Are\n",
      "\\\n",
      "how\n",
      "9a.m\n",
      "could\n",
      "5p.m\n",
      "ಠ_ಠ\n",
      "'S\n",
      "Havin’\n",
      "8a.m\n",
      "┻\n",
      "h\n",
      "0_o\n",
      "'m\n",
      "Nuthin'\n",
      "[=\n",
      "ä.\n",
      "2p.m\n",
      "ö.\n",
      "ca\n",
      "'Coz\n",
      "o’clock\n",
      ":p\n",
      "doin'\n",
      "cuz\n",
      ":-(\n",
      ":-((\n",
      "<\n",
      "ol’\n",
      "Nebr.\n",
      "c'm\n",
      "Cuz\n",
      "o.o\n",
      "Havin\n",
      "a.m\n",
      "those\n",
      "space\n",
      "He\n",
      "Neb\n",
      "Ms.\n",
      "Nev\n",
      "9p.m.\n",
      "n't\n",
      "Ol’\n",
      "¯\\(ツ)/¯\n",
      "were\n",
      "t\n",
      "ta\n",
      "nothin'\n",
      "=)\n",
      "a\n",
      "somethin’\n",
      "dare\n",
      "’cause\n",
      "Lovin’\n",
      "’re\n",
      "goin’\n",
      "'Cos\n",
      "Messrs\n",
      "5\n",
      "/3\n",
      "ll\n",
      "this\n",
      "'y\n",
      "*\n",
      "12p.m\n",
      ":x\n",
      "Jan.\n",
      "Sep.\n",
      "Were\n",
      "Jan\n",
      "]=\n",
      "who's\n",
      "Jul.\n",
      "/\n",
      "2a.m.\n",
      "Doin’\n",
      ":(\n",
      "8D\n",
      "j.\n",
      "°k.\n",
      ":-)))\n",
      "nuthin’\n",
      "O_o\n",
      "O_O\n",
      "nuff\n",
      "(-:\n",
      ":\n",
      "they\n",
      "=/\n",
      "He’s\n",
      "^_^\n",
      "We\n",
      "[-:\n",
      "(:\n",
      "’Coz\n",
      "he\n",
      ":-p\n",
      "He's\n",
      "Let’s\n",
      "Can\n",
      "=[\n",
      "'nuff\n",
      "It's\n",
      "Jun\n",
      "'\n",
      "Lovin'\n",
      "l\n",
      "Lovin\n",
      "when's\n",
      "u\n",
      "Ol\n",
      "Dare\n",
      ":-O\n",
      "q\n",
      ";-)\n",
      "there\n",
      "’nuff\n",
      "havin\n",
      "'bout\n",
      "’Cause\n",
      "Need\n",
      "Somethin\n",
      "gon\n",
      "333\n",
      "N.C.\n",
      "\\n\n",
      "E.G.\n",
      "Ia\n",
      "F.\n",
      "b.\n",
      "v\n",
      "got\n",
      "What’s\n",
      "=3\n",
      ">.>\n",
      ":}\n",
      "Wash\n",
      "3a.m\n",
      "Conn\n",
      "w/o\n",
      "Mass\n",
      "Colo\n",
      ":))\n",
      "1a.m.\n",
      "2p.m.\n",
      "11p.m.\n",
      "It\n",
      "j\n",
      "Those\n",
      "bout\n",
      "Ark.\n",
      "who’s\n",
      "that's\n",
      "Ma’am\n",
      "\\\")\n",
      "’coz\n",
      "</3\n",
      "7a.m.\n",
      "Id.\n",
      "St.\n",
      "Mr\n",
      "=|\n",
      "^___^\n",
      "v_v\n",
      ":’-(\n",
      "Bros\n",
      "(>_<)\n",
      "7a.m\n",
      "Mo\n",
      "D.\n",
      ":'(\n",
      "Ma'am\n",
      "Havin'\n",
      "-O\n",
      "Why\n",
      "(=\n",
      "k.\n",
      "K\n",
      "she’s\n",
      "a.m.\n",
      "Wo\n",
      ":-3\n",
      "Apr.\n",
      "Miss.\n",
      "or\n",
      "when\n",
      "o_0\n",
      "Adm.\n",
      "it's\n",
      "Dr\n",
      "Gen.\n",
      "Is\n",
      "lovin’\n",
      "she\n",
      " \n",
      "Feb.\n",
      "Fla.\n",
      "Id\n",
      "-__-\n",
      "Apr\n",
      "Messrs.\n",
      "ol'\n",
      "<333\n",
      "(-;\n",
      "1a.m\n",
      "may\n",
      "(o:\n",
      "=]\n",
      "C’m\n",
      "all\n",
      "Ltd\n",
      "Doin\n",
      "v.v\n",
      "Fla\n",
      "what\n",
      "Kan.\n",
      "-P\n",
      "Minn.\n",
      "p.m.\n",
      "7\n",
      "and/or\n",
      "Ind\n",
      "Sept\n",
      "‘S\n",
      ":X\n",
      "'s\n",
      "6a.m.\n",
      "N.H.\n",
      "''\n",
      "Sen\n",
      "coz\n",
      "i.\n",
      "nothin\n",
      "Bros.\n",
      "8-D\n",
      ":-D\n",
      "[\n",
      "9a.m.\n",
      "Ark\n",
      "This’s\n",
      "’Cuz\n",
      "8p.m\n",
      "Co\n",
      "10a.m.\n",
      "-x\n",
      "This\n",
      "man\n",
      "Rev\n",
      "Ms\n",
      "re\n",
      "Gon\n",
      "3a.m.\n",
      "There\n",
      "\\t\n",
      "D\n",
      "Ala.\n",
      "12\n",
      "lovin\n",
      "1\n",
      "S\n",
      "There's\n",
      "4p.m\n",
      "Mrs\n",
      "Ol'\n",
      "(._.)\n",
      "Gov\n",
      ":-0\n",
      "Feb\n",
      "N.Y.\n",
      "-p\n",
      "Nuthin\n",
      "nuthin'\n",
      "ma’am\n",
      "):\n",
      "N.D.\n",
      "Corp\n",
      ")\n",
      "These\n",
      "6p.m.\n",
      "ma'am\n",
      "why's\n",
      "-|\n",
      "Ky\n",
      "C++\n",
      "somethin'\n",
      "e.g.\n",
      "-D\n",
      "not\n",
      "Could\n",
      "That\n",
      "Tenn.\n",
      "did\n",
      "(-8\n",
      "w.\n",
      "’Cos\n",
      "I\n",
      "f.\n",
      ":]\n",
      "I.e.\n",
      "°K.\n",
      "’d\n",
      "-3\n",
      "e.\n",
      "Do\n",
      "V.V\n",
      "example\n",
      "Wis\n",
      "Ariz.\n",
      "has\n",
      "Where’s\n",
      "‘\n",
      "’cuz\n",
      "Inc\n",
      "2a.m\n",
      "Md.\n",
      "There’s\n",
      "Ala\n",
      "Ind.\n",
      "°F.\n",
      "Rep.\n",
      ":')\n",
      ";-D\n",
      "have\n",
      "°f.\n",
      "Doin'\n",
      ":>\n",
      "Conn.\n",
      ">\n",
      "She\n",
      "sha\n",
      "y’\n",
      "d.\n",
      "5a.m\n",
      ":O\n",
      "o.O\n",
      "vs\n",
      "╯\n",
      "Did\n",
      ":0\n",
      "7p.m\n",
      "s\n",
      "Nothin'\n",
      "p.\n",
      "-/\n",
      "there’s\n",
      "^__^\n",
      "p\n",
      "'ll\n",
      "She’s\n",
      "'re\n",
      "am\n",
      "'cuz\n",
      "Mo.\n",
      "Corp.\n",
      "Neb.\n",
      "havin’\n",
      "ol\n",
      ">:(\n",
      "<.<\n",
      "Inc.\n",
      "Pa\n",
      "3p.m.\n",
      "°\n",
      ":’(\n",
      ":’)\n",
      "can\n",
      "Jul\n",
      "8\n",
      "Ak.\n",
      "we\n",
      "<33\n",
      "Mt.\n",
      "o_O\n",
      "n\n",
      "em\n",
      "’bout\n",
      "Aug.\n",
      "}\n",
      "(*_*)\n",
      "(╯°□°）╯︵┻━┻\n",
      "on\n",
      "10a.m\n",
      "3\n",
      "Ph\n",
      "How's\n",
      ":/\n",
      "x\n",
      "’ve\n",
      "[:\n",
      "'Cuz\n",
      "Ca\n",
      "that\n",
      "o\n",
      "This's\n",
      ":'-)\n",
      "5a.m.\n",
      "OUTOFVOCAB\n",
      "\t\n",
      "y.\n",
      "St\n",
      "Should\n",
      "6\n",
      "@_@\n",
      "y'\n",
      ":3\n",
      "Mrs.\n",
      "10p.m.\n",
      "Oct\n",
      "7p.m.\n",
      "Md\n",
      "ಠ︵ಠ\n",
      "33\n",
      ";)\n",
      "'d\n",
      "Kans\n",
      ":-(((\n",
      "na\n",
      "i\n",
      "-X\n",
      "xD\n",
      "’ll\n",
      "t.\n",
      "°C.\n",
      "Ak\n",
      "8p.m.\n",
      "'em\n",
      "La.\n",
      ":-X\n",
      "v.\n",
      "Nothin\n",
      "Nuthin’\n",
      ":)\n",
      "z\n",
      "-_-\n",
      "(ಠ_ಠ)\n",
      "’em\n",
      "Ia.\n",
      "Sen.\n",
      ":->\n",
      "How\n",
      "4\n",
      "They\n",
      "4p.m.\n",
      "）\n",
      "how’s\n",
      "ö\n",
      "Dr.\n",
      "2\n",
      "ai\n",
      "let’s\n",
      "Had\n",
      "let's\n",
      "8a.m.\n",
      "goin'\n",
      "o.0\n",
      "r.\n",
      "Got\n",
      "Aug\n",
      "l.\n",
      "XDD\n",
      "P\n",
      "Kans.\n",
      "Cause\n",
      "s.\n",
      "wo\n",
      "Goin\n",
      "C.\n",
      ":-x\n",
      "4a.m\n",
      "u.\n",
      "should\n",
      ":-]\n",
      "E.g\n",
      "’y\n",
      "Ill\n",
      "I.E.\n",
      "=(\n",
      "Wis.\n",
      "are\n",
      "Coz\n",
      "Nebr\n",
      "Adm\n",
      "O.O\n",
      "and\n",
      "Colo.\n",
      "Somethin'\n",
      "sentence\n",
      ":-P\n",
      "Ore\n",
      "—\n",
      "goin\n",
      "\n",
      "\n",
      "let\n",
      "Mar.\n",
      "She's\n",
      "ve\n",
      "E.g.\n",
      ":-|\n",
      ":*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "771"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " for word in nlp.vocab:\n",
    "     print(word.text)\n",
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(embedding):\n",
    "    closest_word = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for word in nlp.vocab:\n",
    "        if not word.has_vector:\n",
    "            continue\n",
    "        similarity = \n",
    "        if similarity > 0.5 and similarity < min_distance:  # Adjust the threshold as needed\n",
    "            closest_word = word.text\n",
    "            min_distance = similarity\n",
    "    \n",
    "    return closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Word: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example word embedding\n",
    "word_embedding = nlp(\"happy\").vector  # Replace \"your_word\" with your actual word\n",
    "\n",
    "# Check if the word vector exists\n",
    "if np.all(word_embedding == 0):\n",
    "    print(\"Error: Word vector not found.\")\n",
    "else:\n",
    "    # Function to calculate cosine similarity\n",
    "    def cosine_similarity(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    # Find the most similar word\n",
    "    most_similar_word = None\n",
    "    max_similarity = -1\n",
    "    for word in nlp.vocab:\n",
    "        if not word.has_vector:\n",
    "            continue\n",
    "        similarity = cosine_similarity(word_embedding, word.vector)\n",
    "        if similarity > max_similarity:\n",
    "            most_similar_word = word.text\n",
    "            max_similarity = similarity\n",
    "\n",
    "    print(\"Most Similar Word:\", most_similar_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Word: glad\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Example word vector (replace this with your actual vector)\n",
    "word_vector = nlp(\"happy\").vector\n",
    "\n",
    "# Sample vocabulary\n",
    "sample_words = [\"joy\", \"glad\", \"content\", \"pleased\", \"cheerful\"]\n",
    "\n",
    "# Find the word with the highest similarity to the given vector\n",
    "most_similar_word = None\n",
    "max_similarity = -1\n",
    "for word in sample_words:\n",
    "    similarity = cosine_similarity(word_vector, nlp(word).vector)\n",
    "    if similarity > max_similarity:\n",
    "        most_similar_word = word\n",
    "        max_similarity = similarity\n",
    "\n",
    "print(\"Most Similar Word:\", most_similar_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
